import os
import asyncio
import time
import uuid
import json
import re
import pandas as pd
import tiktoken
import logging
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union
from contextlib import asynccontextmanager
from tavily import TavilyClient


# GraphRAG ç›¸å…³å¯¼å…¥
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from graphrag.query.indexer_adapters import (
    read_indexer_covariates,
    read_indexer_entities,
    read_indexer_relationships,
    read_indexer_reports,
    read_indexer_text_units,
)
from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings
from graphrag.query.llm.oai.chat_openai import ChatOpenAI
from graphrag.query.llm.oai.embedding import OpenAIEmbedding
from graphrag.query.llm.oai.typing import OpenaiApiType
from graphrag.query.question_gen.local_gen import LocalQuestionGen
from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext
from graphrag.query.structured_search.global_search.search import GlobalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®å¸¸é‡å’Œé…ç½®
INPUT_DIR = os.getenv('INPUT_DIR')
LANCEDB_URI = f"{INPUT_DIR}/lancedb"
COMMUNITY_REPORT_TABLE = "create_final_community_reports"
ENTITY_TABLE = "create_final_nodes"
ENTITY_EMBEDDING_TABLE = "create_final_entities"
RELATIONSHIP_TABLE = "create_final_relationships"
COVARIATE_TABLE = "create_final_covariates"
TEXT_UNIT_TABLE = "create_final_text_units"
COMMUNITY_LEVEL = 2
PORT = 8012

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æœç´¢å¼•æ“å’Œé—®é¢˜ç”Ÿæˆå™¨
local_search_engine = None
global_search_engine = None
question_generator = None


# æ•°æ®æ¨¡å‹
class Message(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: Optional[bool] = False
    stop: Optional[Union[str, List[str]]] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = 0
    frequency_penalty: Optional[float] = 0
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None


class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = None


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: Usage
    system_fingerprint: Optional[str] = None


async def setup_llm_and_embedder():
    """
    è®¾ç½®è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåµŒå…¥æ¨¡å‹
    """
    logger.info("æ­£åœ¨è®¾ç½®LLMå’ŒåµŒå…¥å™¨")

    # è·å–APIå¯†é’¥å’ŒåŸºç¡€URL
    api_key = os.environ.get("GRAPHRAG_API_KEY", "YOUR_API_KEY")
    api_key_embedding = os.environ.get("GRAPHRAG_API_KEY_EMBEDDING", api_key)
    api_base = os.environ.get("API_BASE", "https://api.openai.com/v1")
    api_base_embedding = os.environ.get("API_BASE_EMBEDDING", "https://api.openai.com/v1")

    # è·å–æ¨¡å‹åç§°
    llm_model = os.environ.get("GRAPHRAG_LLM_MODEL", "gpt-3.5-turbo-0125")
    embedding_model = os.environ.get("GRAPHRAG_EMBEDDING_MODEL", "text-embedding-3-small")

    # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦å­˜åœ¨
    if api_key == "YOUR_API_KEY":
        logger.error("ç¯å¢ƒå˜é‡ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„GRAPHRAG_API_KEY")
        raise ValueError("GRAPHRAG_API_KEYæœªæ­£ç¡®è®¾ç½®")

    # åˆå§‹åŒ–ChatOpenAIå®ä¾‹
    llm = ChatOpenAI(
        api_key=api_key,
        api_base=api_base,
        model=llm_model,
        api_type=OpenaiApiType.OpenAI,
        max_retries=20,
    )

    # åˆå§‹åŒ–tokenç¼–ç å™¨
    token_encoder = tiktoken.get_encoding("cl100k_base")

    # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
    text_embedder = OpenAIEmbedding(
        api_key=api_key_embedding,
        api_base=api_base_embedding,
        api_type=OpenaiApiType.OpenAI,
        model=embedding_model,
        deployment_name=embedding_model,
        max_retries=20,
    )


    logger.info("LLMå’ŒåµŒå…¥å™¨è®¾ç½®å®Œæˆ")
    return llm, token_encoder, text_embedder


async def load_context():
    """
    åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®ï¼ŒåŒ…æ‹¬å®ä½“ã€å…³ç³»ã€æŠ¥å‘Šã€æ–‡æœ¬å•å…ƒå’Œåå˜é‡
    """
    logger.info("æ­£åœ¨åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®")
    try:
        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)

        description_embedding_store = LanceDBVectorStore(collection_name="entity_description_embeddings")
        description_embedding_store.connect(db_uri=LANCEDB_URI)
        store_entity_semantic_embeddings(entities=entities, vectorstore=description_embedding_store)

        relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
        relationships = read_indexer_relationships(relationship_df)

        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)

        text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
        text_units = read_indexer_text_units(text_unit_df)

        covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
        claims = read_indexer_covariates(covariate_df)
        logger.info(f"å£°æ˜è®°å½•æ•°: {len(claims)}")
        covariates = {"claims": claims}

        logger.info("ä¸Šä¸‹æ–‡æ•°æ®åŠ è½½å®Œæˆ")
        return entities, relationships, reports, text_units, description_embedding_store, covariates
    except Exception as e:
        logger.error(f"åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        raise


async def setup_search_engines(llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
                               description_embedding_store, covariates):
    """
    è®¾ç½®æœ¬åœ°æœç´¢å¼•æ“å’Œå…¨å±€æœç´¢å¼•æ“
    """
    logger.info("æ­£åœ¨è®¾ç½®æœç´¢å¼•æ“")

    # è®¾ç½®æœ¬åœ°æœç´¢å¼•æ“
    local_context_builder = LocalSearchMixedContext(
        community_reports=reports,
        text_units=text_units,
        entities=entities,
        relationships=relationships,
        covariates=covariates,
        entity_text_embeddings=description_embedding_store,
        embedding_vectorstore_key=EntityVectorStoreKey.ID,
        text_embedder=text_embedder,
        token_encoder=token_encoder,
    )

    local_context_params = {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "conversation_history_user_turns_only": True,
        "top_k_mapped_entities": 10,
        "top_k_relationships": 10,
        "include_entity_rank": True,
        "include_relationship_weight": True,
        "include_community_rank": False,
        "return_candidate_context": False,
        "embedding_vectorstore_key": EntityVectorStoreKey.ID,
        "max_tokens": 12_000,
    }

    local_llm_params = {
        "max_tokens": 2_000,
        "temperature": 0.0,
    }

    local_search_engine = LocalSearch(
        llm=llm,
        context_builder=local_context_builder,
        token_encoder=token_encoder,
        llm_params=local_llm_params,
        context_builder_params=local_context_params,
        response_type="multiple paragraphs",
    )

    # è®¾ç½®å…¨å±€æœç´¢å¼•æ“
    global_context_builder = GlobalCommunityContext(
        community_reports=reports,
        entities=entities,
        token_encoder=token_encoder,
    )

    global_context_builder_params = {
        "use_community_summary": False,
        "shuffle_data": True,
        "include_community_rank": True,
        "min_community_rank": 0,
        "community_rank_name": "rank",
        "include_community_weight": True,
        "community_weight_name": "occurrence weight",
        "normalize_community_weight": True,
        "max_tokens": 12_000,
        "context_name": "Reports",
    }

    map_llm_params = {
        "max_tokens": 1000,
        "temperature": 0.0,
        "response_format": {"type": "json_object"},
    }

    reduce_llm_params = {
        "max_tokens": 2000,
        "temperature": 0.0,
    }

    global_search_engine = GlobalSearch(
        llm=llm,
        context_builder=global_context_builder,
        token_encoder=token_encoder,
        max_data_tokens=12_000,
        map_llm_params=map_llm_params,
        reduce_llm_params=reduce_llm_params,
        allow_general_knowledge=False,
        json_mode=True,
        context_builder_params=global_context_builder_params,
        concurrent_coroutines=32,
        response_type="multiple paragraphs",
    )

    logger.info("æœç´¢å¼•æ“è®¾ç½®å®Œæˆ")
    return local_search_engine, global_search_engine, local_context_builder, local_llm_params, local_context_params


def format_response(response):
    """
    æ ¼å¼åŒ–å“åº”ï¼Œæ·»åŠ é€‚å½“çš„æ¢è¡Œå’Œæ®µè½åˆ†éš”ã€‚
    """
    paragraphs = re.split(r'\n{2,}', response)

    formatted_paragraphs = []
    for para in paragraphs:
        if '```' in para:
            parts = para.split('```')
            for i, part in enumerate(parts):
                if i % 2 == 1:  # è¿™æ˜¯ä»£ç å—
                    parts[i] = f"\n```\n{part.strip()}\n```\n"
            para = ''.join(parts)
        else:
            para = para.replace('. ', '.\n')

        formatted_paragraphs.append(para.strip())

    return '\n\n'.join(formatted_paragraphs)


async def tavily_search(prompt: str):
    """
    ä½¿ç”¨Tavily APIè¿›è¡Œæœç´¢
    """
    try:
        client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])
        resp = client.search(prompt, search_depth="advanced")

        # å°†Tavilyå“åº”è½¬æ¢ä¸ºMarkdownæ ¼å¼
        markdown_response = "# æœç´¢ç»“æœ\n\n"
        for result in resp.get('results', []):
            markdown_response += f"## [{result['title']}]({result['url']})\n\n"
            markdown_response += f"{result['content']}\n\n"

        return markdown_response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tavilyæœç´¢é”™è¯¯: {str(e)}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    global local_search_engine, global_search_engine, question_generator
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–æœç´¢å¼•æ“å’Œé—®é¢˜ç”Ÿæˆå™¨...")
        llm, token_encoder, text_embedder = await setup_llm_and_embedder()
        entities, relationships, reports, text_units, description_embedding_store, covariates = await load_context()
        local_search_engine, global_search_engine, local_context_builder, local_llm_params, local_context_params = await setup_search_engines(
            llm, token_encoder, text_embedder, entities, relationships, reports, text_units,
            description_embedding_store, covariates
        )

        question_generator = LocalQuestionGen(
            llm=llm,
            context_builder=local_context_builder,
            token_encoder=token_encoder,
            llm_params=local_llm_params,
            context_builder_params=local_context_params,
        )
        logger.info("åˆå§‹åŒ–å®Œæˆã€‚")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise

    yield

    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("æ­£åœ¨å…³é—­...")


app = FastAPI(lifespan=lifespan)


# åœ¨ chat_completions å‡½æ•°ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç 

async def full_model_search(prompt: str):
    """
    æ‰§è¡Œå…¨æ¨¡å‹æœç´¢ï¼ŒåŒ…æ‹¬æœ¬åœ°æ£€ç´¢ã€å…¨å±€æ£€ç´¢å’Œ Tavily æœç´¢
    """
    local_result = await local_search_engine.asearch(prompt)
    global_result = await global_search_engine.asearch(prompt)
    tavily_result = await tavily_search(prompt)

    # æ ¼å¼åŒ–ç»“æœ
    formatted_result = "# ğŸ”¥ğŸ”¥ğŸ”¥ç»¼åˆæœç´¢ç»“æœ\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥æœ¬åœ°æ£€ç´¢ç»“æœ\n"
    formatted_result += format_response(local_result.response) + "\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥å…¨å±€æ£€ç´¢ç»“æœ\n"
    formatted_result += format_response(global_result.response) + "\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥Tavily æœç´¢ç»“æœ\n"
    formatted_result += tavily_result + "\n\n"

    return formatted_result


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    if not local_search_engine or not global_search_engine:
        logger.error("æœç´¢å¼•æ“æœªåˆå§‹åŒ–")
        raise HTTPException(status_code=500, detail="æœç´¢å¼•æ“æœªåˆå§‹åŒ–")

    try:
        logger.info(f"æ”¶åˆ°èŠå¤©å®Œæˆè¯·æ±‚: {request}")
        prompt = request.messages[-1].content
        logger.info(f"å¤„ç†æç¤º: {prompt}")

        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„æœç´¢æ–¹æ³•
        if request.model == "graphrag-global-search:latest":
            result = await global_search_engine.asearch(prompt)
            formatted_response = format_response(result.response)
        elif request.model == "tavily-search:latest":
            result = await tavily_search(prompt)
            formatted_response = result
        elif request.model == "full-model:latest":
            formatted_response = await full_model_search(prompt)
        else:  # é»˜è®¤ä½¿ç”¨æœ¬åœ°æœç´¢
            result = await local_search_engine.asearch(prompt)
            formatted_response = format_response(result.response)

        logger.info(f"æ ¼å¼åŒ–çš„æœç´¢ç»“æœ: {formatted_response}")

        # æµå¼å“åº”å’Œéæµå¼å“åº”çš„å¤„ç†ä¿æŒä¸å˜
        if request.stream:
            async def generate_stream():
                chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
                lines = formatted_response.split('\n')
                for i, line in enumerate(lines):
                    chunk = {
                        "id": chunk_id,
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": request.model,
                        "choices": [
                            {
                                "index": 0,
                                "delta": {"content": line + '\n'} if i > 0 else {"role": "assistant", "content": ""},
                                "finish_reason": None
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"
                    await asyncio.sleep(0.05)

                final_chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": request.model,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }
                    ]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(generate_stream(), media_type="text/event-stream")
        else:
            response = ChatCompletionResponse(
                model=request.model,
                choices=[
                    ChatCompletionResponseChoice(
                        index=0,
                        message=Message(role="assistant", content=formatted_response),
                        finish_reason="stop"
                    )
                ],
                usage=Usage(
                    prompt_tokens=len(prompt.split()),
                    completion_tokens=len(formatted_response.split()),
                    total_tokens=len(prompt.split()) + len(formatted_response.split())
                )
            )
            logger.info(f"å‘é€å“åº”: {response}")
            return JSONResponse(content=response.dict())

    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©å®Œæˆæ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/models")
async def list_models():
    """
    è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    logger.info("æ”¶åˆ°æ¨¡å‹åˆ—è¡¨è¯·æ±‚")
    current_time = int(time.time())
    models = [
        {"id": "graphrag-local-search:latest", "object": "model", "created": current_time - 100000, "owned_by": "graphrag"},
        {"id": "graphrag-global-search:latest", "object": "model", "created": current_time - 95000, "owned_by": "graphrag"},
        # {"id": "graphrag-question-generator:latest", "object": "model", "created": current_time - 90000, "owned_by": "graphrag"},
        # {"id": "gpt-3.5-turbo:latest", "object": "model", "created": current_time - 80000, "owned_by": "openai"},
        # {"id": "text-embedding-3-small:latest", "object": "model", "created": current_time - 70000, "owned_by": "openai"},
        {"id": "tavily-search:latest", "object": "model", "created": current_time - 85000, "owned_by": "tavily"},
        {"id": "full-model:latest", "object": "model", "created": current_time - 80000, "owned_by": "combined"}

    ]

    response = {
        "object": "list",
        "data": models
    }

    logger.info(f"å‘é€æ¨¡å‹åˆ—è¡¨: {response}")
    return JSONResponse(content=response)

if __name__ == "__main__":
    import uvicorn

    logger.info(f"åœ¨ç«¯å£ {PORT} ä¸Šå¯åŠ¨æœåŠ¡å™¨")
    uvicorn.run(app, host="0.0.0.0", port=PORT)

