import os
import asyncio
import time
import uuid
import json
import re
import pandas as pd
import tiktoken
import logging
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union
from contextlib import asynccontextmanager
from tavily import TavilyClient
from colorama import init, Fore
# åˆå§‹åŒ– colorama
init(autoreset=True)
# GraphRAG ç›¸å…³å¯¼å…¥
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from graphrag.query.indexer_adapters import (
    read_indexer_covariates,
    read_indexer_entities,
    read_indexer_communities,
    read_indexer_relationships,
    read_indexer_reports,
    read_indexer_text_units,
    read_indexer_report_embeddings
)
from graphrag.query.llm.oai.chat_openai import ChatOpenAI
from graphrag.query.llm.oai.embedding import OpenAIEmbedding
from graphrag.query.llm.oai.typing import OpenaiApiType
from graphrag.query.question_gen.local_gen import LocalQuestionGen
from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext
from graphrag.query.structured_search.global_search.search import GlobalSearch
from graphrag.query.structured_search.drift_search.drift_context import (
    DRIFTSearchContextBuilder,
)
from graphrag.config.models.drift_search_config import DRIFTSearchConfig
from graphrag.query.structured_search.drift_search.search import DRIFTSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
# Athene-V2-Chat_exl2_2.25bpwï¼ŒRombos-Coder-V2.5-Qwen-32b-exl2_5.0bpw
LLM_MODEL = "Rombos-LLM-V2.5-Qwen-32b-4.5bpw-exl2"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®å¸¸é‡å’Œé…ç½®
INPUT_DIR = "E:\\graphrag_kb\\input\\artifacts"
LANCEDB_URI = "E:\\graphrag_kb\\output\\lancedb"
COMMUNITY_REPORT_TABLE = "create_final_community_reports"
FINAL_COMMUNITY_TABLE = "create_final_communities"
ENTITY_TABLE = "create_final_nodes"
ENTITY_EMBEDDING_TABLE = "create_final_entities"
RELATIONSHIP_TABLE = "create_final_relationships"
COVARIATE_TABLE = "create_final_covariates"
TEXT_UNIT_TABLE = "create_final_text_units"
COMMUNITY_LEVEL = 10     # ç¤¾åŒºå±‚çº§ï¼Œè¶Šé«˜è¡¨ç¤ºä½¿ç”¨æ›´ç²¾ç»†çš„ç¤¾åŒºæŠ¥å‘Šï¼ˆä½†è®¡ç®—æˆæœ¬æ›´é«˜ï¼‰ï¼Œé»˜è®¤2
PORT = 8013

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æœç´¢å¼•æ“å’Œé—®é¢˜ç”Ÿæˆå™¨
local_search_engine = None
global_search_engine = None
drift_serch_engine = None
question_generator = None


# æ•°æ®æ¨¡å‹
class Message(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.5
    top_p: Optional[float] = 0.7
    n: Optional[int] = 1
    stream: Optional[bool] = True
    stop: Optional[Union[str, List[str]]] = None
    max_tokens: Optional[int] = 12_000
    presence_penalty: Optional[float] = 0
    frequency_penalty: Optional[float] = 0
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None


class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = None


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: Usage
    system_fingerprint: Optional[str] = None


async def setup_llm_and_embedder():
    """
    è®¾ç½®è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåµŒå…¥æ¨¡å‹
    çŸ¥è¯†å›¾è°±æ— æ³•æ­£å¸¸ä½¿ç”¨æ—¶ç”¨gemini-1.5-flash-latestæ¨¡å‹é‡æ–°è®­ç»ƒå°æ–‡æ¡£ç”Ÿæˆå›¾è°±
    """
    """
    # è·å–APIå¯†é’¥å’ŒåŸºç¡€URL
    api_key = "xxx"
    api_key_embedding = "xxx"
    api_base = "https://ai.liaobots.work/v1"
    api_base_embedding = "https://ai.liaobots.work/v1"
    api_base = "http://localhost:11434/v1"
    # è·å–æ¨¡å‹åç§°
    LLM_MODEL = "gpt-4o-mini"
    embedding_model = "text-embedding-ada-002"
    """
    logger.info("æ­£åœ¨è®¾ç½®LLMå’ŒåµŒå…¥å™¨")
    # ollamaè·å–APIå¯†é’¥å’ŒåŸºç¡€URL
    api_key = "xxx"
    api_base = "http://127.0.0.1:5001/v1"
    global LLM_MODEL
    logger.info(Fore.CYAN + f"GRAPHRAGä½¿ç”¨æ¨¡å‹ï¼š{LLM_MODEL}")
    # åˆå§‹åŒ–ChatOpenAIå®ä¾‹
    llm = ChatOpenAI(
        api_key=api_key,
        api_base=api_base,
        model=LLM_MODEL,
        api_type=OpenaiApiType.OpenAI,
        max_retries=10,
        request_timeout=120  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º120ç§’
    )

    # åˆå§‹åŒ–tokenç¼–ç å™¨
    token_encoder = tiktoken.get_encoding("cl100k_base")

    # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
    # openaiåœ¨çº¿æ¨¡å‹
    """
    api_key="sk-9mxwRPHwHt8M1ct7CaCf041d6fC44e9587A041Ca3145E09e",
    api_base="https://apis.wumingai.com/v1",
    model=embedding_model,
    deployment_name=embedding_model,
    """
    # xinferenceæœ¬åœ°åµŒå…¥æ¨¡å‹
    """
    api_key="xinference",
    api_base="http://127.0.0.1:9997/v1",
    model="bge-m3",
    deployment_name="bge-m3",
    """
    text_embedder = OpenAIEmbedding(
        # æœ¬åœ°åµŒå…¥æ¨¡å‹
        api_key="ollama",
        api_base="http://localhost:11434/v1",
        model="bge-m3:Q4",
        deployment_name="bge-m3:Q4",
        api_type=OpenaiApiType.OpenAI,
        max_retries=20,
    )

    logger.info("LLMå’ŒåµŒå…¥å™¨è®¾ç½®å®Œæˆ")
    return llm, token_encoder, text_embedder

def embed_text(column):
    text_embedder = OpenAIEmbedding(
        # æœ¬åœ°åµŒå…¥æ¨¡å‹
        api_key="ollama",
        api_base="http://localhost:11434/v1",
        model="bge-m3:Q4",
        deployment_name="bge-m3:Q4",
        api_type=OpenaiApiType.OpenAI,
        max_retries=20,
    )

    return column.apply(lambda x: text_embedder.embed(x))
async def load_context():
    """
    åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®ï¼ŒåŒ…æ‹¬å®ä½“ã€å…³ç³»ã€æŠ¥å‘Šã€æ–‡æœ¬å•å…ƒå’Œåå˜é‡
    """
    logger.info("æ­£åœ¨åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®")
    try:
        entity_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_TABLE}.parquet")
        entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet")
        entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)

        description_embedding_store = LanceDBVectorStore(collection_name="default-entity-description")
        description_embedding_store.connect(db_uri=LANCEDB_URI)

        relationship_df = pd.read_parquet(f"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet")
        relationships = read_indexer_relationships(relationship_df)

        report_df = pd.read_parquet(f"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet")
        #reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL,content_embedding_col="full_content_embeddings")
        reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL,content_embedding_col="full_content_embeddings")
        full_content_embedding_store = LanceDBVectorStore(collection_name="default-community-full_content")
        full_content_embedding_store.connect(db_uri=LANCEDB_URI)
        read_indexer_report_embeddings(reports, full_content_embedding_store)
        
        final_communities_df = pd.read_parquet(f"{INPUT_DIR}/{FINAL_COMMUNITY_TABLE}.parquet")
        communities = read_indexer_communities(final_communities_df,entity_df,report_df)


        text_unit_df = pd.read_parquet(f"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet")
        text_units = read_indexer_text_units(text_unit_df)
        covariate_df = pd.read_parquet(f"{INPUT_DIR}/{COVARIATE_TABLE}.parquet")
        claims = read_indexer_covariates(covariate_df)
        logger.info(f"å£°æ˜è®°å½•æ•°: {len(claims)}")
        covariates = {"claims": claims}

        logger.info("ä¸Šä¸‹æ–‡æ•°æ®åŠ è½½å®Œæˆ")
        return entities, relationships, reports, communities,text_units, description_embedding_store, covariates
    except Exception as e:
        logger.error(f"åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        raise


async def setup_search_engines(llm, token_encoder, text_embedder, entities, relationships, reports,communities, text_units,
                            description_embedding_store, covariates):
    """
    è®¾ç½®æœ¬åœ°æœç´¢å¼•æ“å’Œå…¨å±€æœç´¢å¼•æ“
    """
    logger.info("æ­£åœ¨è®¾ç½®æœç´¢å¼•æ“")

    # è®¾ç½®æœ¬åœ°æœç´¢å¼•æ“
    local_context_builder = LocalSearchMixedContext(
        community_reports=reports,
        text_units=text_units,
        entities=entities,
        relationships=relationships,
        covariates=covariates,
        entity_text_embeddings=description_embedding_store,
        embedding_vectorstore_key=EntityVectorStoreKey.ID,
        text_embedder=text_embedder,
        token_encoder=token_encoder,
    )

    local_context_params = {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "conversation_history_user_turns_only": True,
        "top_k_mapped_entities": 10,
        "top_k_relationships": 10,
        "include_entity_rank": True,
        "include_relationship_weight": True,
        "include_community_rank": False,
        "return_candidate_context": False,
        "embedding_vectorstore_key": EntityVectorStoreKey.ID,
        "max_tokens": 12_000,
    }

    local_llm_params = {
        "max_tokens": 12_000,
        "temperature": 0.3,
    }

    local_search_engine = LocalSearch(
        llm=llm,
        context_builder=local_context_builder,
        token_encoder=token_encoder,
        llm_params=local_llm_params,
        context_builder_params=local_context_params,
        response_type="multiple paragraphs",
    )

    # è®¾ç½®å…¨å±€æœç´¢å¼•æ“
    global_context_builder = GlobalCommunityContext(
        communities = communities ,
        community_reports=reports,
        entities=entities,
        token_encoder=token_encoder,
    )

    global_context_builder_params = {
        "use_community_summary": False,
        "shuffle_data": True,
        "include_community_rank": True,
        "min_community_rank": 0.5,
        "community_rank_name": "rank",
        "include_community_weight": True,
        "community_weight_name": "occurrence weight",
        "normalize_community_weight": True,
        "max_tokens": 12_000,
        "context_name": "Reports",
    }

    map_llm_params = {
        "max_tokens": 12_000,
        "temperature": 0.5,
        "response_format": {"type": "json_object"},
    }

    reduce_llm_params = {
        "max_tokens": 12_000,
        "temperature": 0.5,
    }

    global_search_engine = GlobalSearch(
        llm=llm,
        context_builder=global_context_builder,
        token_encoder=token_encoder,
        max_data_tokens=12_000,
        map_llm_params=map_llm_params,
        reduce_llm_params=reduce_llm_params,
        allow_general_knowledge=True,
        json_mode=True,
        context_builder_params=global_context_builder_params,
        concurrent_coroutines=32,
        response_type="multiple paragraphs",
    )
    drift_params = DRIFTSearchConfig()
    drift_params.temperature = 0.5
    drift_params.max_tokens = 12_000
    drift_context_builder = DRIFTSearchContextBuilder(
            chat_llm=llm,
            text_embedder=text_embedder,
            entities=entities,
            relationships=relationships,
            reports=reports,
            entity_text_embeddings=description_embedding_store,
            text_units=text_units,
            config = drift_params

        )
    drift_serch_engine = DRIFTSearch(
            llm=llm, context_builder=drift_context_builder, token_encoder=token_encoder
        )
    logger.info("æœç´¢å¼•æ“è®¾ç½®å®Œæˆ")
    return local_search_engine, global_search_engine,drift_serch_engine, local_context_builder, local_llm_params, local_context_params


def format_response(response):
    """
    æ ¼å¼åŒ–å“åº”ï¼Œæ·»åŠ é€‚å½“çš„æ¢è¡Œå’Œæ®µè½åˆ†éš”ã€‚
    """
    modified_text = re.sub(r"`(.*?)`", r'```python\1```', response)

    return modified_text


async def tavily_search(prompt: str):
    """
    ä½¿ç”¨Tavily APIè¿›è¡Œæœç´¢
    """
    try:
        client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])
        resp = client.search(prompt, search_depth="advanced")

        # å°†Tavilyå“åº”è½¬æ¢ä¸ºMarkdownæ ¼å¼
        markdown_response = "# æœç´¢ç»“æœ\n\n"
        for result in resp.get('results', []):
            markdown_response += f"## [{result['title']}]({result['url']})\n\n"
            markdown_response += f"{result['content']}\n\n"

        return markdown_response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tavilyæœç´¢é”™è¯¯: {str(e)}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    global local_search_engine, global_search_engine,drift_serch_engine, question_generator
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–æœç´¢å¼•æ“å’Œé—®é¢˜ç”Ÿæˆå™¨...")
        llm, token_encoder, text_embedder = await setup_llm_and_embedder()
        entities, relationships, reports,communities, text_units, description_embedding_store, covariates = await load_context()
        local_search_engine, global_search_engine,drift_serch_engine, local_context_builder, local_llm_params, local_context_params = await setup_search_engines(
            llm, token_encoder, text_embedder, entities, relationships, reports,communities, text_units,
            description_embedding_store, covariates
        )

        question_generator = LocalQuestionGen(
            llm=llm,
            context_builder=local_context_builder,
            token_encoder=token_encoder,
            llm_params=local_llm_params,
            context_builder_params=local_context_params,
        )
        logger.info("åˆå§‹åŒ–å®Œæˆã€‚")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise

    yield

    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("æ­£åœ¨å…³é—­...")


app = FastAPI(lifespan=lifespan)


# åœ¨ chat_completions å‡½æ•°ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç 

async def full_model_search(prompt: str):
    """
    æ‰§è¡Œå…¨æ¨¡å‹æœç´¢ï¼ŒåŒ…æ‹¬æœ¬åœ°æ£€ç´¢ã€å…¨å±€æ£€ç´¢å’Œ Tavily æœç´¢
    """
    local_result = await local_search_engine.asearch(prompt)
    global_result = await global_search_engine.asearch(prompt)
    drift_result = await drift_serch_engine.asearch(prompt)
    tavily_result = await tavily_search(prompt)

    # æ ¼å¼åŒ–ç»“æœ
    formatted_result = "# ğŸ”¥ğŸ”¥ğŸ”¥ç»¼åˆæœç´¢ç»“æœ\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥æœ¬åœ°æ£€ç´¢ç»“æœ\n"
    formatted_result += local_result.response + "\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥å…¨å±€æ£€ç´¢ç»“æœ\n"
    formatted_result += global_result.response + "\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥æ··åˆæ£€ç´¢ç»“æœ\n"
    formatted_result += drift_result.response + "\n\n"

    formatted_result += "## ğŸ”¥ğŸ”¥ğŸ”¥Tavily æœç´¢ç»“æœ\n"
    formatted_result += tavily_result + "\n\n"

    return formatted_result

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    if not local_search_engine or not global_search_engine or not drift_serch_engine:
        logger.error("æœç´¢å¼•æ“æœªåˆå§‹åŒ–")
        raise HTTPException(status_code=500, detail="æœç´¢å¼•æ“æœªåˆå§‹åŒ–")

    prompt = request.messages[-1].content
    logger.info(Fore.CYAN + f"æ”¶åˆ°æ¨¡å‹è¯·æ±‚å†…å®¹ï¼š{prompt}")
    # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„æœç´¢æ–¹æ³•
    if request.model == "graphrag-global-search:latest":
        result = await global_search_engine.asearch(prompt)
        formatted_response = result.response
    elif request.model == "graphrag-drift-search:latest":
        result = await drift_serch_engine.asearch(prompt)
        formatted_response = result.response
        formatted_response:str = formatted_response["nodes"][0]["answer"]
        formatted_response = formatted_response.replace(" n n","\n")
    elif request.model == "tavily-search:latest":
        result = await tavily_search(prompt)
        formatted_response = result
    elif request.model == "full-model:latest":
        formatted_response = await full_model_search(prompt)
    else:  # é»˜è®¤ä½¿ç”¨æœ¬åœ°æœç´¢
        result = await local_search_engine.asearch(prompt)
        # æ ¼å¼åŒ–å›å¤
        #formatted_response = format_response(result.response)
        formatted_response = result.response

    logger.info(Fore.CYAN + f"çŸ¥è¯†å›¾è°±çš„æœç´¢ç»“æœ: {formatted_response}")
    # æµå¼å“åº”å’Œéæµå¼å“åº”çš„å¤„ç†ä¿æŒä¸å˜
    if request.stream:
        async def generate_stream():
            chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
            lines = formatted_response.split('\n')
            for i, line in enumerate(lines):
                chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": request.model,
                    "choices": [
                        {
                            "index": 0,
                            #"delta": {"content": line + '\n'} if i > 0 else {"role": "assistant", "content": ""},
                            "delta": {"content": line + '\n'}, # if i > 0 else {"role": "assistant", "content": ""},
                            "finish_reason": None
                        }
                    ]
                }
                yield f"data: {json.dumps(chunk)}\n\n"
                await asyncio.sleep(0.05)

            final_chunk = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": request.model,
                "choices": [
                    {
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }
                ]
            }
            yield f"data: {json.dumps(final_chunk)}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(generate_stream(), media_type="text/event-stream")
    else:
        response = ChatCompletionResponse(
            model=request.model,
            choices=[
                ChatCompletionResponseChoice(
                    index=0,
                    message=Message(role="assistant", content=formatted_response),
                    finish_reason="stop"
                )
            ],
            usage=Usage(
                prompt_tokens=len(prompt.split()),
                completion_tokens=len(formatted_response.split()),
                total_tokens=len(prompt.split()) + len(formatted_response.split())
            )
        )
        logger.info(f"å‘é€å“åº”: {response}")
        return JSONResponse(content=response.dict())

@app.get("/v1/models")
async def list_models():
    """
    è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    logger.info("æ”¶åˆ°æ¨¡å‹åˆ—è¡¨è¯·æ±‚")
    current_time = int(time.time())
    models = [
        {"id": "graphrag-local-search:latest", "object": "model", "created": current_time - 100000, "owned_by": "graphrag"},
        {"id": "graphrag-global-search:latest", "object": "model", "created": current_time - 95000, "owned_by": "graphrag"},
        {"id": "graphrag-drift-search:latest", "object": "model", "created": current_time - 90000, "owned_by": "graphrag"},
        # {"id": "graphrag-question-generator:latest", "object": "model", "created": current_time - 90000, "owned_by": "graphrag"},
        # {"id": "gpt-3.5-turbo:latest", "object": "model", "created": current_time - 8_0000, "owned_by": "openai"},
        # {"id": "text-embedding-3-small:latest", "object": "model", "created": current_time - 70000, "owned_by": "openai"},
        #{"id": "tavily-search:latest", "object": "model", "created": current_time - 85000, "owned_by": "tavily"},
        # {"id": "full-model:latest", "object": "model", "created": current_time - 8_0000, "owned_by": "combined"}

    ]

    response = {
        "object": "list",
        "data": models
    }

    logger.info(f"å‘é€æ¨¡å‹åˆ—è¡¨: {response}")
    return JSONResponse(content=response)

if __name__ == "__main__":
    import uvicorn

    logger.info(f"åœ¨ç«¯å£ {PORT} ä¸Šå¯åŠ¨æœåŠ¡å™¨")
    uvicorn.run(app, host="0.0.0.0", port=PORT)

